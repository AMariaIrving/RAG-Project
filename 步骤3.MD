好的，我们现在进入第三阶段的设计。这个阶段是系统的“资源获取”中心，它将拿着第二阶段精心准备的“购物清单”（`structured_req_matrix.jsonl`），去两个不同的“市场”采购原材料。一个市场是“经过认证的品牌专卖店”（RAG），另一个是“充满灵感的创意集市”（LLM）。

我们的设计目标是确保这个采购过程**高效、并行、且结果清晰分离**，为第四阶段的“烹饪”提供最高质量的食材。

---

### `run_stage_3.py` 内部逻辑的超详细设计

**脚本总体目标:** 读取 `/02_stage_structuring/structured_req_matrix.jsonl`，为其中的每一条需求，并行地从RAG系统和LLM模型获取解决方案素材，并分别存入位于 `/03_stage_generation/` 文件夹下的 `a_rag_snippets.jsonl` 和 `b_llm_suggestions.jsonl` 文件中。

---

#### **步骤 1: 环境初始化与输入读取 (Setup & Ingestion)**

*   **功能描述:** 准备工作环境，加载“购物清单”。
*   **详细流程:**
    1.  **路径定义:** 脚本内部定义输入文件路径 (`../02_stage_structuring/structured_req_matrix.jsonl`) 和输出路径 (`../03_stage_generation/`)。
    2.  **输出目录检查:** 检查输出路径是否存在，如果不存在，则创建。
    3.  **输入文件加载:** 脚本打开 `structured_req_matrix.jsonl` 文件，**逐行读取**，并将每一行解析为一个JSON对象。将所有这些对象加载到一个Python的列表（List of Dictionaries）中，方便后续遍历。

---

#### **步骤 2: 并行任务分发与执行 (Parallel Task Dispatch & Execution)**

*   **功能描述:** 这是本阶段的核心执行逻辑。为了最大化效率，我们将为输入列表中的每一条需求，创建两个并行的子任务（一个给RAG，一个给LLM），然后并发地执行它们。
*   **详细流程:**
    1.  **创建任务队列:** 脚本遍历【步骤1】中加载的需求列表。对于列表中的**每一个需求对象（我们称之为`req_item`）**，在内存中创建两个任务：
        *   **任务A (RAG检索):** 标记为 `rag_task`，关联 `req_item`。
        *   **任务B (LLM生成):** 标记为 `llm_task`，关联 `req_item`。
    2.  **使用并发框架:** 脚本将使用Python的并发库（如 `asyncio` 用于IO密集型任务，或 `concurrent.futures.ThreadPoolExecutor`）来管理和执行这些任务队列。这能确保在等待一个API返回结果时，程序可以去处理另一个API的请求，从而大大缩短总执行时间。
    3.  **执行并收集结果:** 框架会分发并执行每一个任务。脚本需要等待所有任务都执行完毕，并将每个任务的返回结果（成功或失败）与原始的 `req_id` 关联起来，存储在一个临时的结果字典中。

---

#### **子流程 2.A: RAG精确检索任务的内部逻辑 (The Factual Path Task)**

*   **功能描述:** 这是每一个`rag_task`内部需要执行的具体动作。
*   **详细流程:**
    1.  **获取输入:** 任务从其关联的`req_item`中，提取出以下信息：
        *   `req_id`
        *   `query_sentence`
        *   `metadata`
    2.  **调用RAG API:**
        *   **主查询:** 将 `query_sentence` 作为核心查询参数，调用您的RAG系统的检索接口。
        *   **后置过滤 (Post-filtering):** 在API调用中，如果您的RAG系统支持，将`metadata`（如 `domain: ["质量"]`, `system_level: ["MES"]`）作为过滤条件一并传入。这能让RAG在向量检索之后，再进行一次精确的标签匹配，极大地提高返回结果的精准度。如果RAG系统不支持API层面的过滤，则需要在下一步手动进行。
    3.  **结果处理:**
        *   接收RAG系统返回的文档片段列表（例如，返回Top 5个最相关的片段）。
        *   **手动过滤 (如果需要):** 如果上一步没有进行后置过滤，现在就需要在这里用代码对返回的片段进行筛选。
        *   **格式化:** 将最终筛选出的片段，整理成我们在设计阶段定义的标准格式，包含 `source`, `page`, `content` 等字段。
    4.  **返回结果:** 任务返回一个包含 `req_id` 和格式化后的 `rag_snippets` 列表的Python字典。

---

#### **子流程 2.B: LLM创造性生成任务的内部逻辑 (The Creative Path Task)**

*   **功能描述:** 这是每一个`llm_task`内部需要执行的具体动作。
*   **详细流程:**
    1.  **获取输入:** 任务从其关联的`req_item`中，提取出：
        *   `req_id`
        *   `query_sentence`
        *   `source_description` (作为背景)
    2.  **Prompt构建:** 动态地为当前这一个需求，构建一个专用的Prompt。
        *   **角色扮演:** "你是一位经验丰富且富有创造力的行业解决方案架构师。"
        *   **上下文注入:**
            *   "【背景】: [此处插入`source_description`]"
            *   "【核心需求】: [此处插入`query_sentence`]"
        *   **核心任务指令:** "请针对以上核心需求，提出三种具有前瞻性的、方向性的解决方案思路。你的回答不应涉及具体的产品品牌，而是侧重于方法论、技术路径或策略层面。请分析每种思路的优缺点。"
        *   **输出格式约束:** "请以JSON格式返回你的答案，根对象应包含'option_1', 'option_2', 'option_3'三个键，每个键的值是一个包含'title', 'description', 'keywords'等字段的对象。"
    3.  **API调用:** 调用一个强大的MOE/LLM的API来执行这个任务。
    4.  **结果解析与验证:**
        *   接收API返回的JSON字符串。
        *   尝试用JSON解析器进行解析，以验证格式的正确性。同样，可以加入重试逻辑。
    5.  **返回结果:** 任务返回一个包含 `req_id` 和解析后的 `llm_suggestions` 对象的Python字典。

---

#### **步骤 3: 结果汇总与持久化 (Aggregation & Persistence)**

*   **功能描述:** 在所有并发任务完成后，将收集到的结果分类并写入对应的输出文件。
*   **详细流程:**
    1.  **分类整理:** 脚本遍历【步骤2】中收集到的所有任务结果。
        *   如果结果来自 `rag_task`，则将其放入一个名为 `all_rag_results` 的列表中。
        *   如果结果来自 `llm_task`，则将其放入另一个名为 `all_llm_results` 的列表中。
    2.  **写入RAG结果文件:** 脚本打开 `/03_stage_generation/a_rag_snippets.jsonl` 文件。遍历 `all_rag_results` 列表，将每一个结果（它本身就是一个字典）转换为JSON字符串，并作为一行写入文件。
    3.  **写入LLM结果文件:** 脚本打开 `/03_stage_generation/b_llm_suggestions.jsonl` 文件。遍历 `all_llm_results` 列表，同样地，将每一个结果转换为JSON字符串，并作为一行写入文件。

---

**人工审核点（再次强调）:**
*   **您的动作:** 脚本执行完毕。现在 `/03_stage_generation/` 文件夹下有了两个新的“食材库”。
*   **审核清单:**
    1.  **打开 `a_rag_snippets.jsonl`:** 抽查几条需求，看看RAG返回的片段是否真的相关？来源是否可靠？是不是一些无用的“噪音”？
    2.  **打开 `b_llm_suggestions.jsonl`:** 抽查几条需求，看看LLM提出的方向性建议是否有启发性？是否过于离谱？有没有出现明显的“幻觉”？
*   **您的权力:** 虽然不建议直接修改这两个文件（因为它们是“原始食材”），但您的审核结果将是**判断您的RAG系统质量和LLM模型选择是否得当**的**最直接反馈**。如果发现RAG检索效果很差，您就知道需要去优化RAG的索引或embedding模型。如果LLM经常胡说八道，您就知道可能需要更换模型或调整Prompt中的“温度”参数。